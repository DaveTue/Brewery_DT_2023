@article{brand2021,
   abstract = {In recent years, digital twin (DT) technology has moved to the center of attention of many researchers and engineers. Commonly, a digital twin is defined based on a virtual entity (VE) that exhibits similar behavior to its physical counterpart, and that is coupled to this physical entity (PE). The VE thus forms a core part of any digital twin. While VEs may differ vastly - from ones based on simple simulation to high-fidelity virtual mirroring of the corresponding PE - they are typically composed of multiple models that may originate from multiple domains, address different aspects, and are expressed and processed using different tools and languages. Furthermore, the use of time series data - whether historical or real-time or both - from the PE distinguishes VEs from mere simulations. As a consequence of the modeling landscape complexity and the data aspect of VEs, the design of a digital twin and specifically of the VE as part of it represents several challenges. In this paper, we present our vision for the development, evolution, maintenance, and verification of such virtual entities for digital twins.},
   author = {Mark van den Brand and Loek Cleophas and Raghavendran Gunasekaran and Boudewijn Haverkort and David A.Manrique Negrin and Hossain Muhammad Muctadir},
   doi = {10.1109/MODELS-C53483.2021.00039},
   isbn = {9781665424844},
   journal = {Companion Proceedings - 24th International Conference on Model-Driven Engineering Languages and Systems, MODELS-C 2021},
   keywords = {digital twin,digital twin development roadmap,dynamic consistency,model consistency,model management,model orchestration},
   pages = {225-228},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Models Meet Data: Challenges to Create Virtual Entities for Digital Twins},
   year = {2021},
}
@article{Negrin2021,
   abstract = {The concept of Digital Twin (DT) is increasingly getting attention due to its support for digital transformation as part of Industry 4.0. A core component of a DT is the virtual entity (VE) that is meant to 'mirror' the physical entity (PE). A particular challenge for developing this VE is the integration between the different models that typically make up such a VE. These models, covering different disciplines or aspects of a system, commonly are developed by different engineers and expressed in different formalisms given their strengths and weaknesses. This makes integration between these models and formalisms a challenge. Such integration has two aspects: the communication among the models, and their orchestration, which concerns the order of model step execution and data exchange. In this paper, we consider the suitability of Ptolemy II, an open source framework based on an actor-oriented structure, to implement such integration. We use the Ptolemy II framework to reproduce the implementation of a DT of an autonomous scaled-down truck, which originally required a more manual configuration and execution and was developed in a monolithic way. The results show that Ptolemy II can be used to implement different communication technologies as actors, and show the expressiveness of the orchestrators by reproducing the existing DT's behavior. This shows Ptolemy II in principle to be suitable as an integration and orchestration tool for models in the context of DTs. As future work, we plan to apply the framework to more complex DTs using different communication technologies, so as to validate its broader suitability for VE orchestration in DTs.},
   author = {David A.Manrique Negrin and Loek Cleophas and Mark van den Brand},
   doi = {10.1109/MODELS-C53483.2021.00041},
   isbn = {9781665424844},
   journal = {Companion Proceedings - 24th International Conference on Model-Driven Engineering Languages and Systems, MODELS-C 2021},
   keywords = {Digital twin,Ptolemy II,framework,integration,orchestration},
   pages = {233-236},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Using {Ptolemy} {II} as a Framework for Virtual Entity Integration and Orchestration in Digital Twins},
   year = {2021},
}
@article{Tao2017,
   abstract = {With the developments and applications of the new information technologies, such as cloud computing, Internet of Things, big data, and artificial intelligence, a smart manufacturing era is coming. At the same time, various national manufacturing development strategies have been put forward, such as Industry 4.0, Industrial Internet, manufacturing based on Cyber-Physical System, and Made in China 2025. However, one of specific challenges to achieve smart manufacturing with these strategies is how to converge the manufacturing physical world and the virtual world, so as to realize a series of smart operations in the manufacturing process, including smart interconnection, smart interaction, smart control and management, etc. In this context, as a basic unit of manufacturing, shop-floor is required to reach the interaction and convergence between physical and virtual spaces, which is not only the imperative demand of smart manufacturing, but also the evolving trend of itself. Accordingly, a novel concept of digital twin shop-floor (DTS) based on digital twin is explored and its four key components are discussed, including physical shop-floor, virtual shop-floor, shop-floor service system, and shop-floor digital twin data. What is more, the operation mechanisms and implementing methods for DTS are studied and key technologies as well as challenges ahead are investigated, respectively.},
   author = {Fei Tao and Meng Zhang},
   doi = {10.1109/ACCESS.2017.2756069},
   issn = {21693536},
   journal = {IEEE Access},
   keywords = {Smart manufacturing,convergence,cyber-physical system (CPS),digital twin,digital twin shop-floor (DTS),shop-floor digital twin data (SDTD),shop-floor service system (SSS),virtual shop-floor (VS)},
   month = {9},
   pages = {20418-20427},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Digital Twin Shop-Floor: A New Shop-Floor Paradigm Towards Smart Manufacturing},
   volume = {5},
   year = {2017},
}
@article{Liu2021,
   abstract = {Various kinds of engineering software and digitalized equipment are widely applied through the lifecycle of industrial products. As a result, massive data of different types are being produced. However, these data are hysteretic and isolated from each other, leading to low efficiency and low utilization of these valuable data. Simulation based on theoretical and static model has been a conventional and powerful tool for the verification, validation, and optimization of a system in its early planning stage, but no attention is paid to the simulation application during system run-time. With the development of new-generation information and digitalization technologies, more data can be collected, and it is time to find a way for the deep application of all these data. As a result, the concept of digital twin has aroused much concern and is developing rapidly. Dispute and discussions around concepts, paradigms, frameworks, applications, and technologies of digital twin are on the rise both in academic and industrial communities. After a complete search of several databases and careful selection according to the proposed criteria, 240 academic publications about digital twin are identified and classified. This paper conducts a comprehensive and in-depth review of these literatures to analyze digital twin from the perspective of concepts, technologies, and industrial applications. Research status, evolution of the concept, key enabling technologies of three aspects, and fifteen kinds of industrial applications in respective lifecycle phase are demonstrated in detail. Based on this, observations and future work recommendations for digital twin research are presented in the form of different lifecycle phases.},
   author = {Mengnan Liu and Shuiliang Fang and Huiyue Dong and Cunzhi Xu},
   doi = {10.1016/J.JMSY.2020.06.017},
   issn = {0278-6125},
   journal = {Journal of Manufacturing Systems},
   keywords = {Digital twin,Industrial application,Literature review,Product lifecycle,Simulation},
   month = {1},
   pages = {346-361},
   publisher = {Elsevier},
   title = {Review of digital twin about concepts, technologies, and industrial applications},
   volume = {58},
   year = {2021},
}
@article{Udugama2021,
   abstract = {The domain of industrial biomanufacturing is enthusiastically embracing the concept of Digital Twin, owing to its promises of increased process efficiency and resource utilisation. However, Digital Twin in biomanufacturing is not yet clearly defined and this sector of the industry is falling behind the others in terms of its implementation. On the other hand, some of the benefits of Digital Twin seem to overlap with the more established practices of process control and optimization, and the term is vaguely used in different scenarios. In an attempt to clarify this issue, we investigate this overlap for the specific case of fermentation operation, a central step in many biomanufacturing processes. Based on this investigation, a framework built upon a five-step pathway starting from a basic steady-state process model is proposed to develop a fully-fledged Digital Twin. For demonstration purposes, the framework is applied to a bench-scale second-generation ethanol fermentation process as a case study. It is proposed that the success or failure of a fully-fledged Digital Twin implementation is determined by key factors that comprise the role of modelling, human operator actions, and other propositions of economic value.},
   author = {Isuru A Udugama and Pau C Lopez and Carina L Gargalo and Xueliang Li and Christoph Bayer and Krist V Gernaey},
   doi = {10.1007/s43393-021-00024-0},
   issn = {2662-7663},
   issue = {3},
   journal = {Systems Microbiology and Biomanufacturing},
   pages = {257-274},
   title = {Digital Twin in biomanufacturing: challenges and opportunities towards its implementation},
   volume = {1},
   url = {https://doi.org/10.1007/s43393-021-00024-0},
   year = {2021},
}
@article{Mears2017,
   abstract = {A novel model-based control strategy has been developed for filamentous fungal fed-batch fermentation processes. The system of interest is a pilot scale (550 L) filamentous fungus process operating at Novozymes A/S. In such processes, it is desirable to maximize the total product achieved in a batch in a defined process time. In order to achieve this goal, it is important to maximize both the product concentration, and also the total final mass in the fed-batch system. To this end, we describe the development of a control strategy which aims to achieve maximum tank fill, while avoiding oxygen limited conditions. This requires a two stage approach: (i) calculation of the tank start fill; and (ii) on-line control in order to maximize fill subject to oxygen transfer limitations. First, a mechanistic model was applied off-line in order to determine the appropriate start fill for processes with four different sets of process operating conditions for the stirrer speed, headspace pressure, and aeration rate. The start fills were tested with eight pilot scale experiments using a reference process operation. An on-line control strategy was then developed, utilizing the mechanistic model which is recursively updated using on-line measurements. The model was applied in order to predict the current system states, including the biomass concentration, and to simulate the expected future trajectory of the system until a specified end time. In this way, the desired feed rate is updated along the progress of the batch taking into account the oxygen mass transfer conditions and the expected future trajectory of the mass. The final results show that the target fill was achieved to within 5% under the maximum fill when tested using eight pilot scale batches, and over filling was avoided. The results were reproducible, unlike the reference experiments which show over 10% variation in the final tank fill, and this also includes over filling. The variance of the final tank fill is reduced by over 74%, meaning that it is possible to target the final maximum fill reproducibly. The product concentration achieved at a given set of process conditions was unaffected by the control strategy. Biotechnol. Bioeng. 2017;114: 1459–1468. © 2017 Wiley Periodicals, Inc.},
   author = {Lisa Mears and Stuart M. Stocks and Mads O. Albaek and Benny Cassells and Gürkan Sin and Krist V. Gernaey},
   doi = {10.1002/BIT.26274},
   issn = {1097-0290},
   issue = {7},
   journal = {Biotechnology and Bioengineering},
   keywords = {based control,batch,control,fed,fermentation,model,modelling,process optimization},
   month = {7},
   pages = {1459-1468},
   pmid = {28240344},
   publisher = {John Wiley & Sons, Ltd},
   title = {A novel model-based control strategy for aerobic filamentous fungal fed-batch fermentation processes},
   volume = {114},
   url = {https://onlinelibrary.wiley.com/doi/full/10.1002/bit.26274},
   year = {2017},
}
@article{Kritzinger2018,
   abstract = {The Digital Twin (DT) is commonly known as a key enabler for the digital transformation, however, in literature is no common understanding concerning this term. It is used slightly different over the disparate disciplines. The aim of this paper is to provide a categorical literature review of the DT in manufacturing and to classify existing publication according to their level of integration of the DT. Therefore, it is distinct between Digital Model (DM), Digital Shadow (DS) and Digital Twin. The results are showing, that literature concerning the highest development stage, the DT, is scarce, whilst there is more literature about DM and DS.},
   author = {Werner Kritzinger and Matthias Karner and Georg Traar and Jan Henjes and Wilfried Sihn},
   doi = {10.1016/J.IFACOL.2018.08.474},
   issn = {2405-8963},
   issue = {11},
   journal = {IFAC-PapersOnLine},
   keywords = {Digital Model,Digital Shadow,Digital Twin,Literature Review,Manufacturing,Production},
   month = {1},
   pages = {1016-1022},
   publisher = {Elsevier},
   title = {Digital Twin in manufacturing: A categorical literature review and classification},
   volume = {51},
   year = {2018},
}
@article{Narayanan2020,
   abstract = {In this age of technology, the vision of manufacturing industries built of smart factories is not a farfetched future. As a prerequisite for Industry 4.0, industrial sectors are moving towards digitalization and automation. Despite its tremendous growth reaching a sales value of worth $188 billion in 2017, the biopharmaceutical sector distinctly lags in this transition. Currently, the challenges are innovative market disruptions such as personalized medicine as well as increasing commercial pressure for faster and cheaper product manufacturing. Improvements in digitalization and data analytics have been identified as key strategic activities for the next years to face these challenges. Alongside, there is an emphasis by the regulatory authorities on the use of advanced technologies, proclaimed through initiatives such as Quality by Design (QbD) and Process Analytical Technology (PAT). In the manufacturing sector, the biopharmaceutical domain features some of the most complex and least understood processes. Thereby, process models that can transform process data into more valuable information, guide decision-making, and support the creation of digital and automated technologies are key enablers. This review summarizes the current state of model-based methods in different bioprocess related applications and presents the corresponding future vision for the biopharmaceutical industry to achieve the goals of Industry 4.0 while meeting the regulatory requirements.},
   author = {Harini Narayanan and Martin F. Luna and Moritz von Stosch and Mariano Nicolas Cruz Bournazou and Gianmarco Polotti and Massimo Morbidelli and Alessandro Butté and Michael Sokolov},
   doi = {10.1002/BIOT.201900172},
   issn = {1860-7314},
   issue = {1},
   journal = {Biotechnology Journal},
   keywords = {bioprocesses,digitalization,industry 4.0,predictive models,process analytical technology},
   month = {1},
   pages = {1900172},
   pmid = {31486583},
   publisher = {John Wiley & Sons, Ltd},
   title = {Bioprocessing in the Digital Age: The Role of Process Models},
   volume = {15},
   url = {https://onlinelibrary.wiley.com/doi/full/10.1002/biot.201900172},
   year = {2020},
}
@article{Lopez2020,
   abstract = {Background: The diauxic growth of Saccharomyces cerevisiae on glucose and xylose during cellulose-to-ethanol processes extends the duration of the fermentation and reduces productivity. Despite the remarkable advances in strain engineering, the co-consumption of glucose and xylose is still limited due to catabolite repression. This work addresses this challenge by developing a closed-loop controller that is capable of maintaining the glucose concentration at a steady set-point during fed-batch fermentation. The suggested controller uses a data-driven model to measure the concentration of glucose from ‘real-time’ spectroscopic data. The concentration of glucose is then automatically controlled using a control scheme that consists of a proportional, integral, differential (PID) algorithm and a supervisory layer that manipulates the feed-rates to the reactor accounting for the changing dynamics of fermentation. Results: The PID parameters and the supervisory layer were progressively improved throughout four fed-batch lignocellulosic-to-ethanol fermentations to attain a robust controller able of maintaining the glucose concentration at the pre-defined set-points. The results showed an increased co-consumption of glucose and xylose that resulted in volumetric productivities that are 20–33% higher than the reference batch processes. It was also observed that fermentations operated at a glucose concentration of 10 g/L were faster than those operated at 4 g/L, indicating that there is an optimal glucose concentration that maximises the overall productivity. Conclusions: Promoting the simultaneous consumption of glucose and xylose in S. cerevisiae is critical to increase the productivity of lignocellulosic ethanol processes, but also challenging due to the strong catabolite repression of glucose on the uptake of xylose. Operating the fermentation at low concentrations of glucose allows reducing the effects of the catabolite repression to promote the co-consumption of the two carbon sources. However, S. cerevisiae is very sensitive to changes in the glucose concentration and deviations from a set-point result in notable productivity losses. The controller structure developed and implemented in this work illustrates how combining data-driven measurements of the glucose concentration and a robust yet effective PID-based supervisory control allowed tight control of the concentration of glucose to adjust it to the metabolic requirements of the cell culture that can unlock tangible gains in productivities.},
   author = {Pau Cabaneros Lopez and Isuru Abeykoon Udugama and Sune Tjalfe Thomsen and Christoph Bayer and Helena Junicke and Krist V. Gernaey},
   doi = {10.1186/S13068-020-01829-2},
   issn = {17546834},
   issue = {1},
   journal = {Biotechnology for Biofuels},
   keywords = {Biotechnology,Environmental Engineering/Biotechnology,Microbiology,Plant Breeding/Biotechnology,Renewable and Green Energy},
   month = {12},
   pages = {1-14},
   publisher = {BioMed Central Ltd},
   title = {Promoting the co-utilisation of glucose and xylose in lignocellulosic ethanol fermentations using a data-driven feed-back controller},
   volume = {13},
   url = {https://biotechnologyforbiofuels.biomedcentral.com/articles/10.1186/s13068-020-01829-2},
   year = {2020},
}
@article{Udugama2020,
   abstract = {With the emergence of Industry 4.0 and Big Data initiatives, there is a renewed interest in leveraging the vast amounts of data collected in (bio)chemical processes to improve their operations. The objective of this article is to provide a perspective of the current status of Big-Data-based process control methodologies and the most effective path to further embed these methodologies in the control of (bio)chemical processes. Therefore, this article provides an overview of operational requirements, the availability and the nature of data, and the role of the control structure hierarchy in (bio)chemical processes and how they constrain this endeavor. The current state of the seemingly competing methodologies of statistical process monitoring and (engineering) process control is examined together with hybrid methodologies that are attempting to combine tools and techniques that belong to either camp. The technical and economic considerations of a deeper integration between the two approaches is then explored, and a path forward is proposed.},
   author = {Isuru A. Udugama and Carina L. Gargalo and Yoshiyuki Yamashita and Michael A. Taube and Ahmet Palazoglu and Brent R. Young and Krist V. Gernaey and Murat Kulahci and Christoph Bayer},
   doi = {10.1021/ACS.IECR.0C01872/ASSET/IMAGES/ACS.IECR.0C01872.SOCIAL.JPEG_V03},
   issn = {15205045},
   issue = {34},
   journal = {Industrial and Engineering Chemistry Research},
   month = {8},
   pages = {15283-15297},
   publisher = {American Chemical Society},
   title = {The Role of Big Data in Industrial (Bio)chemical Process Operations},
   volume = {59},
   url = {https://pubs.acs.org/doi/full/10.1021/acs.iecr.0c01872},
   year = {2020},
}
@article{Gargalo2020,
   abstract = {The bio-manufacturing industry, along with other process industries, now has the opportunity to be engaged in the latest industrial revolution, also known as Industry 4.0. To successfully accomplish this, a physical-to-digital-to-physical information loop should be carefully developed. One way to achieve this is, for example, through the implementation of digital twins (DTs), which are virtual copies of the processes. Therefore, in this paper, the focus is on understanding the needs and challenges faced by the bio-manufacturing industry when dealing with this digitalized paradigm. To do so, two major building blocks of a DT, data and models, are highlighted and discussed. Hence, firstly, data and their characteristics and collection strategies are examined as well as new methods and tools for data processing. Secondly, modelling approaches and their potential of being used in DTs are reviewed. Finally, we share our vision with regard to the use of DTs in the bio-manufacturing industry aiming at bringing the DT a step closer to its full potential and realization.},
   author = {Carina L. Gargalo and Simoneta Caño de Las Heras and Mark Nicholas Jones and Isuru Udugama and Seyed Soheil Mansouri and Ulrich Krühne and Krist V. Gernaey},
   doi = {10.1007/10_2020_142},
   issn = {07246145},
   journal = {Advances in biochemical engineering/biotechnology},
   keywords = {Bio-manufacturing,Bioprocess modelling,Data processing,Digital twins},
   pages = {1-34},
   pmid = {33349908},
   publisher = {Springer, Cham},
   title = {Towards the Development of Digital Twins for the Bio-manufacturing Industry},
   volume = {176},
   url = {https://link.springer.com/chapter/10.1007/10_2020_142},
   year = {2020},
}
@article{Preuveneers2018,
   abstract = {Industry 4.0 is an emerging business paradigm that is reaping the benefits of enabling technologies driving intelligent systems and environments. By acquiring, processing and acting upon various kinds of relevant context information, smart automated manufacturing systems can make well-informed decisions to adapt and optimize their production processes at runtime. To manage this complexity, the manufacturing world is proposing the 'Digital Twin' model to represent physical products in the real space and their virtual counterparts in the virtual space, with data connections to tie the virtual and real products together for an augmented view of the manufacturing workflow. The benefits of such representations are simplified process simulations and efficiency optimizations, predictions, early warnings, etc. However, the robustness and fidelity of digital twins are a critical concern, especially when independently developed production systems and corresponding digital twins interfere with one another in a manufacturing workflow and jeopardize the proper behavior of production systems. We therefore evaluate the addition of safeguards to digital twins for smart cyber-physical production systems (CPPS) in an Industry 4.0 manufacturing workflow in the form of feature toggles that are managed at runtime by software circuit breakers. Our evaluation shows how these improvements can increase the robustness of interacting digital twins by avoiding local errors from cascading through the distributed production or manufacturing workflow.},
   author = {Davy Preuveneers and Wouter Joosen and Elisabeth Ilie-Zudor},
   doi = {10.1109/EDOCW.2018.00021},
   isbn = {9781538641415},
   issn = {15417719},
   journal = {Proceedings - IEEE International Enterprise Distributed Object Computing Workshop, EDOCW},
   keywords = {Circuit Breaker,Cyber-Physical Production Systems,Digital Twin,Feature Toggle},
   month = {11},
   pages = {69-78},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Robust Digital Twin Compositions for Industry 4.0 Smart Manufacturing Systems},
   volume = {2018-October},
   year = {2018},
}
@article{Hung2022,
   abstract = {Many core technologies of Industry 4.0 have gained substantial advancement in recent years. Digital Twin (DT) has become the key technology and tool for manufacturing industries to realize intelligent cyber-physical integration and digital transformation by leveraging these technologies. Although there have been many DT-related works, there is no standard definition, unified framework, and implementation approach of DT until now. Widely developing DTs for the manufacturing industry is still challenging. Thus, this paper proposes a novel implementation framework of digital twins for intelligent manufacturing, denoted as IF-DTiM, which possesses several distinct merits to distinguish itself from previous works. First, IF-DTiM fully utilizes new-generation container technology so that DT-related applications and services can be packaged in a self-contained way, rapidly deployed, and robustly operated with the capabilities of failover, autoscaling, and load balancing. Second, it leverages existing intelligent cloud manufacturing services to realize the intelligence for DT externally in a scalable and plug-and-play manner instead of using traditional approaches to embed intelligence in DT. Third, IF-DTiM contains Product DT for products, Equipment DT (i.e., EQ DT) for equipment, and Process DT for production lines, which can generically fulfill the demands and scenarios to achieve intelligent manufacturing for various manufacturing industries. Testing results show that IF-DTiM can achieve remarkable performance in rapid deployment and real-time data exchanges of DT-related applications. Finally, we develop an example DTiM system for CNC machining based on IF-DTiM to demonstrate its efficacy and applicability in facilitating the manufacturing industry to build their DT systems.},
   author = {Min Hsiung Hung and Yu Chuan Lin and Hung Chang Hsiao and Chao Chun Chen and Kuan Chou Lai and Yu Ming Hsieh and Hao Tieng and Tsung Han Tsai and Hsien Cheng Huang and Haw Ching Yang and Fan Tien Cheng},
   doi = {10.1109/TASE.2022.3143832},
   issn = {15583783},
   journal = {IEEE Transactions on Automation Science and Engineering},
   keywords = {Cloud computing,Containers,Digital twin,Fourth Industrial Revolution,Implementation framework,Manufacturing,Manufacturing industries,Software,container technology.,digital twins,intelligent manufacturing},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {A Novel Implementation Framework of Digital Twins for Intelligent Manufacturing Based on Container Technology and Cloud Manufacturing Services},
   year = {2022},
}
@article{Borghesi2021,
   abstract = {With the increase of the volume of data produced by IoT devices, there is a growing demand of applications capable of elaborating data anywhere along the IoT-to-Cloud path (Edge/Fog). In industrial environments, strict real-time constraints require computation to run as close to the data origin as possible (e.g., IoT Gateway or Edge nodes), whilst batch-wise tasks such as Big Data analytics and Machine Learning model training are advised to run on the Cloud, where computing resources are abundant. The H2020 IoTwins project leverages the digital twin concept to implement virtual representation of physical assets (e.g., machine parts, machines, production/control processes) and deliver a software platform that will help enterprises, and in particular SMEs, to build highly innovative, AI-based services that exploit the potential of IoT/Edge/Cloud computing paradigms. In this paper, we discuss the design principles of the IoTwins reference architecture, delving into technical details of its components and offered functionalities, and propose an exemplary software implementation.},
   author = {Andrea Borghesi and Giuseppe Di Modica and Paolo Bellavista and Varun Gowtham and Alexander Willner and Daniel Nehls and Florian Kintzler and Stephan Cejka and Simone Rossi Tisbeni and Alessandro Costantini and Matteo Galletti and Marica Antonacci and Jean Christian Ahouangonou},
   doi = {10.1109/CCGRID51090.2021.00075},
   isbn = {9781728195865},
   journal = {Proceedings - 21st IEEE/ACM International Symposium on Cluster, Cloud and Internet Computing, CCGrid 2021},
   keywords = {Cloud,Digital Twins,Edge,IoT,IoTwins,Service Orchestration},
   month = {5},
   pages = {625-633},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {IoTwins: Design and implementation of a platform for the management of digital twins in industrial scenarios},
   year = {2021},
}
@article{Krone2020,
   abstract = {Conceptual design based on superstructure optimization is a complex task that neither commercial simulators nor dedicated modeling and optimization environments like GAMS, AMPL, and AIMMS are able to perform well by themselves: the first lack interfaces to state-of-the-art solvers, the latter do not provide accurate thermodynamic models. While previous research shows that GAMS can be interfaced with an external thermo engine, this interfacing requires additional C ++ code, whose manual generation and consistent implementation is tedious, making the approach both error prone and impractical for larger design problems. By using MOSAICmodeling as a modeling environment, this shortcoming of the existing approach is eliminated by automatically generating all code needed for interfacing GAMS with an external CAPE-OPEN thermodynamic property package.},
   author = {David Krone and Erik Esche and Norbert Asprion and Mirko Skiborowski and Jens Uwe Repke},
   doi = {10.1016/B978-0-12-823377-1.50146-4},
   issn = {1570-7946},
   journal = {Computer Aided Chemical Engineering},
   keywords = {CAPE-OPEN,automated code generation,conceptual design},
   month = {1},
   pages = {15-20},
   publisher = {Elsevier},
   title = {Conceptual Design Based on Superstructure Optimization in GAMS with Accurate Thermodynamic Models},
   volume = {48},
   year = {2020},
}
@article{Karolius2018,
   author = {Sigve Karolius and Heinz Preisig},
   doi = {10.3384/ECP18153210},
   journal = {Proceedings of The 59th Conference on imulation and Modelling (SIMS 59), 26-28 September 2018, Oslo Metropolitan University, Norway},
   month = {11},
   pages = {210-215},
   publisher = {Linköping University Electronic Press},
   title = {Developing Simulation Tools for Interdisciplinary Modelling},
   volume = {153},
   year = {2018},
}
@article{Belaud2002,
   abstract = {Traditionally simulation environments have been closed monolithic systems and the resulting bottlenecks in interoperability, reuse and innovation have led to the search for a more open and interoperable solution. The CAPE-OPEN (CO) effort, launched in January, 1997, is a standardisation process for achieving true plug and play of process industry simulation software components. The resulting CO standard is now being widely disseminated to the chemical engineering community. It relies on a technology that integrates up to date concepts from the software field such as a component-based approach. A number of software components based on this technology have been developed and are already available. Thanks to this new generation of CAPE tools, it is expected to reach cheaper, better and faster design, operation and control of processes. The CAPE-OPEN Laboratories Network (CO-LaN) consortium is in charge of managing the lifecycle of the CO standard. © 2002 Elsevier B.V. All rights reserved.},
   author = {Jean Pierre Belaud and Michel Pons},
   doi = {10.1016/S1570-7946(02)80169-9},
   issn = {1570-7946},
   issue = {C},
   journal = {Computer Aided Chemical Engineering},
   month = {1},
   pages = {847-852},
   publisher = {Elsevier},
   title = {Open Software Architecture For Process Simulation: The Current Status of CAPE-OPEN Standard},
   volume = {10},
   year = {2002},
}
@article{Blochwitz2011,
   abstract = {The Functional Mockup Interface (FMI) is a tool independent standard for the exchange of dynamic models and for co-simulation. The development of FMI was initiated and organized by Daimler AG within the ITEA2 project MODELISAR. The prima- ry goal is to support the exchange of simulation models between suppliers and OEMs even if a large variety of different tools are used. The FMI was de- veloped in a close collaboration between simulation tool vendors and research institutes. In this article an overview about FMI is given and technical details about the solution are discussed.},
   author = {T. Blochwitz and M. Otter and M. Arnold and C. Bausch and C. Clauss and H. Elmqvist and A. Junghanns and J. Mauss and M. Monteiro and T. Neidhold and D. Neumerkel and H. Olsson and J.-V. Peetz and S. Wolf},
   doi = {10.3384/ECP11063105},
   journal = {Proceedings from the 8th International Modelica Conference, Technical Univeristy, Dresden, Germany},
   month = {6},
   pages = {105-114},
   publisher = {Linköping University Electronic Press},
   title = {The Functional Mockup Interface for Tool independent Exchange of Simulation Models},
   volume = {63},
   year = {2011},
}
@article{Neema2014,
   abstract = {Virtual evaluation of complex Cyber -Physical Systems (CPS) [1] with a number of tightly integrated domains such as physical, mechanical, electrical, thermal, cyber, etc. demand the use of heterogeneous simulation environments. Our previous effort with C2 Wind Tunnel (C2WT) [2] [3] attempted to solve the challenges of evaluating these complex systems as - a -whole , by integrating multiple simulation platforms with varying semantics and integrating and managing different simulation models and their interactions. Recently, a great interest has developed to use Functional Mockup Interface (FMI) [4 ] for a variety of dynamics simulation packages, particularly in the automotive industry. Leveraging the C2WT effort on effective integration of different simulation engines with different Models of Computation (MoCs), we propose, in this paper, to use the proven methods of High -Level Architecture (HLA) -based model and system integration. We identify the challenges of integrating Functional Mockup Unit for Co -Simulation (FMU -CS) in general and via HLA [5] and present a novel model -based approach to rapidly synthesize an effective integration. The approach presented provides a unique opportunity to integrate readily available FMU -CS components with various specialized simulation packages to rapidly synthesize HLA-based integrated simulations for the overall composed Cyber -Physical Systems.},
   author = {Himanshu Neema and Jesse Gohl and Zsolt Lattmann and Janos Sztipanovits and Gabor Karsai and Sandeep Neema and Ted Bapty and John Batteh and Hubertus Tummescheit and Chandrasekar Sureshkumar},
   doi = {10.3384/ECP14096235},
   journal = {Proceedings of the 10th International Modelica Conference, March 10-12, 2014, Lund, Sweden},
   month = {3},
   pages = {235-245},
   publisher = {Linköping University Electronic Press},
   title = {Model-Based Integration Platform for FMI Co-Simulation and Heterogeneous Simulations of Cyber-Physical Systems},
   volume = {96},
   year = {2014},
}
@article{Tolksdorf2016,
   abstract = {This contribution deals with the integration of custom unit operations into flowsheeting software. By sketching the workflow of designing novel processes in a conventional simulation software and introducing a respective modeling scenario some challenges regarding collaboration and model-solution are named: the exchange of models, the definition of initial values, and the ordering of equations. A strategy to overcome these challenges is presented and applied in case studies using a collaborative modeling and code generation tool and a CAPE OPEN-compliant unit operation framework. These examples show not only the advantages of the CAPE-OPEN standard for interoperability in process science but also the advantages of automatic code generation based on mathematical analysis of the equation systems.},
   author = {Gregor Tolksdorf and Erik Esche and Jasper van Baten and Gnteür Wozny},
   doi = {10.1016/B978-0-444-63428-3.50136-3},
   isbn = {9780444634283},
   issn = {15707946},
   journal = {Computer Aided Chemical Engineering},
   keywords = {Automatic Code Generation,CAPE-OPEN,Collaborative Engineering,Efficient Numerical Solution},
   pages = {787-792},
   publisher = {Elsevier B.V.},
   title = {Taylor-Made Modeling and Solution of Novel Process Units by Modular CAPE-OPEN-based Flowsheeting},
   volume = {38},
   url = {http://dx.doi.org/10.1016/B978-0-444-63428-3.50136-3},
   year = {2016},
}
@book{Ptolemaeus2014,
   author = {Claudius Ptolemaeus},
   publisher = {Ptolemy.org },
   title = {System design, modeling, and simulation: using Ptolemy II},
   volume = {1},
   year = {2014},
}
@article{Eppinger2021,
   abstract = {The food industry has improved product quality while reducing production time and cost by automating production using programmable logic controllers (PLC) over the last several decades. However, many production plants still require some level of manual expert interaction, mainly because the production processes are not 100% under control. Operators are often still present to take quality samples, re-tune unit operation controls or resolve failures.The use of a physics-based “Digital Twin” is getting more and more traction to develop the equipment virtually due to the improvements in prediction accuracy and speed of computation. Digital twins allow engineers to find the optimal design before the unit goes into production. However, these digital twins can’t be deployed at the operational level because they can be complex or too slow to react at the speed of operation.In this contribution a new set of solutions that lowers the barrier in executing the digital twins on the production floor is explain based on a few examples. This will deliver substantial return on investment (ROI) for the food production industry. They include technologies such as:A machine learning based methodology to perform Model Order Reduction (MOR) on the digital twin in order to get real time response based on production information.A machine learning based methodology to convert the reduced model into a virtual sensor for online quality predictions or predictive maintenance scheduling as well as to use it for creating an optimal controller of the unit based on the product requirements.Fast edge computing hardware that can collect data from sensors and run the Executable Digital Twin (xDT) to suggest corrective action to the operator, in real time, or ultimately run in closed loop control.},
   author = {Thomas Eppinger and Glenn Longwell and Peter Mas and Kevin Goodheart and Umberto Badiali and Ravindra Aglave},
   doi = {10.3303/CET2187007},
   isbn = {978-88-95608-85-3},
   issn = {2283-9216},
   journal = {Chemical Engineering Transactions},
   month = {7},
   pages = {37-42},
   publisher = {Italian Association of Chemical Engineering - AIDIC},
   title = {Increase Food Production Efficiency Using the Executable Digital Twin (xdt)},
   volume = {87},
   url = {https://www.cetjournal.it/index.php/cet/article/view/CET2187007},
   year = {2021},
}
@web_page{ibm,
   author = {IBM},
   title = {What is a digital twin?},
   url = {https://www.ibm.com/topics/what-is-a-digital-twin},
}
@web_page{pwc2014,
   author = {PwC},
   title = {Industry 4.0 - Publications},
   url = {https://www.pwc.nl/en/publicaties/industrie-4-0.html},
}
@article{Schluse2018,
   abstract = {Digital twins represent real objects or subjects with their data, functions, and communication capabilities in the digital world. As nodes within the internet of things, they enable networking and thus the automation of complex value-added chains. The application of simulation techniques brings digital twins to life and makes them experimentable; digital twins become experimentable digital twins (EDTs). Initially, these EDTs communicate with each other purely in the virtual world. The resulting networks of interacting EDTs model different application scenarios and are simulated in virtual testbeds, providing new foundations for comprehensive simulation-based systems engineering. Its focus is on EDTs, which become more detailed with every single application. Thus, complete digital representations of the respective real assets and their behaviors are created successively. The networking of EDTs with real assets leads to hybrid application scenarios in which EDTs are used in combination with real hardware, thus realizing complex control algorithms, innovative user interfaces, or mental models for intelligent systems.},
   author = {Michael Schluse and Marc Priggemeyer and Linus Atorf and Juergen Rossmann},
   doi = {10.1109/TII.2018.2804917},
   issn = {15513203},
   issue = {4},
   journal = {IEEE Transactions on Industrial Informatics},
   keywords = {eRobotics,experimentable digital twin (EDT),intelligent systems,simulation-based X, virtual testbed (VTB),simulation-based systems engineering},
   month = {4},
   pages = {1722-1731},
   publisher = {IEEE Computer Society},
   title = {Experimentable Digital Twins-Streamlining Simulation-Based Systems Engineering for Industry 4.0},
   volume = {14},
   year = {2018},
}
@article{Shafto2012,
   author = {Mike Shafto and Mike Conroy and Rich Doyle and Ed Glaessgen and Chris Kemp and Jacqueline LeMoigne and Lui Wang},
   issue = {2012},
   journal = {National Aeronautics and Space Administration},
   pages = {1-38},
   title = {Modeling, simulation, information technology and processing roadmap},
   volume = {32},
   year = {2012},
}
@article{Rasheed2020,
   abstract = {Digital twin can be defined as a virtual representation of a physical asset enabled through data and simulators for real-time prediction, optimization, monitoring, controlling, and improved decision making. Recent advances in computational pipelines, multiphysics solvers, artificial intelligence, big data cybernetics, data processing and management tools bring the promise of digital twins and their impact on society closer to reality. Digital twinning is now an important and emerging trend in many applications. Also referred to as a computational megamodel, device shadow, mirrored system, avatar or a synchronized virtual prototype, there can be no doubt that a digital twin plays a transformative role not only in how we design and operate cyber-physical intelligent systems, but also in how we advance the modularity of multi-disciplinary systems to tackle fundamental barriers not addressed by the current, evolutionary modeling practices. In this work, we review the recent status of methodologies and techniques related to the construction of digital twins mostly from a modeling perspective. Our aim is to provide a detailed coverage of the current challenges and enabling technologies along with recommendations and reflections for various stakeholders.},
   author = {Adil Rasheed and Omer San and Trond Kvamsdal},
   doi = {10.1109/ACCESS.2020.2970143},
   issn = {21693536},
   journal = {IEEE Access},
   keywords = {Digital twin,artificial intelligence,big data cybernetics,hybrid analysis and modeling,machine learning},
   pages = {21980-22012},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Digital twin: Values, challenges and enablers from a modeling perspective},
   volume = {8},
   year = {2020},
}
@article{Barricelli2020,
   abstract = {Our research work describes a team of human Digital Twins (DTs), each tracking fitness-related measurements describing an athlete's behavior in consecutive days (e.g. food income, activity, sleep). After collecting enough measurements, the DT firstly predicts the physical twin performance during training and, in case of non-optimal result, it suggests modifications in the athlete's behavior. The athlete's team is integrated into SmartFit, a software framework for supporting trainers and coaches in monitoring and manage athletes' fitness activity and results. Through IoT sensors embedded in wearable devices and applications for manual logging (e.g. mood, food income), SmartFit continuously captures measurements, initially treated as the dynamic data describing the current physical twins' status. Dynamic data allows adapting each DT's status and triggering the DT's predictions and suggestions. The analyzed measurements are stored as the historical data, further processed by the DT to update (increase) its knowledge and ability to provide reliable predictions. Results show that, thanks to the team of DTs, SmartFit computes trustable predictions of the physical twins' conditions and produces understandable suggestions which can be used by trainers to trigger optimization actions in the athletes' behavior. Though applied in the sport context, SmartFit can be easily adapted to other monitoring tasks.},
   author = {Barbara Rita Barricelli and Elena Casiraghi and Jessica Gliozzo and Alessandro Petrini and Stefano Valtolina},
   doi = {10.1109/ACCESS.2020.2971576},
   issn = {21693536},
   journal = {IEEE Access},
   keywords = {Counterfactual explanations,Digital twins,Internet of Things,Machine learning,Smart health,Sociotechnical design,Wearables},
   pages = {26637-26664},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Human Digital Twin for Fitness Management},
   volume = {8},
   year = {2020},
}
@web_page{tensorflow,
   title = {TensorFlow},
   url = {https://www.tensorflow.org/},
}
@web_page{pytorch,
   title = {PyTorch},
   url = {https://pytorch.org/},
}
@article{Rutan1990,
   abstract = {The use of recursive filtering techniques for parameter estimation in a variety of areas is reviewed. In particular, the Kalman filter algorithm is described, along with several variations, including square-root, UDUT and information filters. The solution to parameter estimation problems is discussed for both linear and non-linear models. Applications described include calibration, curve resolution in spectroscopy, chromatography, electrochemistry, kinetic analysis and process monitoring.},
   author = {Sarah C. Rutan},
   doi = {10.1002/CEM.1180040203},
   issn = {1099-128X},
   issue = {2},
   journal = {Journal of Chemometrics},
   keywords = {Information filter,Kalman filter,Parameter estimation,Recursive digital filter,Square,root filter},
   month = {3},
   pages = {103-121},
   publisher = {John Wiley & Sons, Ltd},
   title = {Recursive parameter estimation},
   volume = {4},
   url = {https://onlinelibrary.wiley.com/doi/full/10.1002/cem.1180040203},
   year = {1990},
}
@article{Oisiovici2000,
   abstract = {Composition monitoring and control play an essential role during a batch distillation cycle, but on-line composition analyzers are expensive, difficult to maintain and give delayed responses. Considering the need and lack of a stochastic estimator for batch distillation columns, a discrete extended Kalman filter (EKF) for binary and multicomponent systems has been developed and tested. The aim of the EKF was to provide reliable and real-time column composition profiles from few temperature measurements and easily available information. Accurate composition estimates and fast convergence were obtained, and the EKF has confirmed its ability to incorporate the effects of noise (from both measurement and modeling). The number of sensors and the observation frequency have shown to be important variables in the design of the EKF, especially for systems with fast dynamics. (C) 2000 Elsevier Science Ltd. All rights reserved.},
   author = {R. M. Oisiovici and S. L. Cruz},
   doi = {10.1016/S0009-2509(00)00088-9},
   issn = {0009-2509},
   issue = {20},
   journal = {Chemical Engineering Science},
   keywords = {Batch distillation,Discrete systems,Inference,Kalman filtering,Nonlinear,Stochastic estimator},
   month = {10},
   pages = {4667-4680},
   publisher = {Pergamon},
   title = {State estimation of batch distillation columns using an extended Kalman filter},
   volume = {55},
   year = {2000},
}
@article{Kramer2016,
   abstract = {In the early process development, feeding trajectories and cultivation conditions are altered in order to maximise the desired product. This is in contrast to industrial cultivations, where variations are to be eliminated. In order to fully understand and safely run a new process, real-time information are beneficial. Near-infrared spectrometer that are coupled to a fermenter offer the possibility to gather information on-line. The NIR data can be transformed via partial least squares modeling to estimate substrate and biomass concentrations. At low concentrations and under changed cultivation conditions during process development, however, the estimates may differ from reality. This uncertainty may be reduced by integrating biological and physico-chemical knowledge in the on-line estimation. In this contribution, we present a hybrid approach of near-infrared (NIR) spectroscopy and nonlinear model-based state estimation to enable an improved quality in the on-line estimation of substrates and biomass in a yeast cultivation. As only three cultivations are needed for calibration, on-line state estimation is available in the early stage of development for this process. This approach is compared to the use of both methods separately for estimation of biomass, ammonium, glucose, phosphate and ethanol in cultivations of S. cerevisiae.},
   author = {Dominik Krämer and Rudibert King},
   doi = {10.1016/J.IFACOL.2016.07.235},
   issn = {2405-8963},
   issue = {7},
   journal = {IFAC-PapersOnLine},
   keywords = {EKF,NIR spectroscopy,PLS,bio-control,cultivation process,hybrid modeling,nonlinear state estimation},
   month = {1},
   pages = {609-614},
   publisher = {Elsevier},
   title = {On-line monitoring of substrates and biomass using near-infrared spectroscopy and model-based state estimation for enzyme production by S. cerevisiae},
   volume = {49},
   year = {2016},
}
@web_page{pidni,
   title = {{PID} Theory Explained - {NI}},
   url = {https://www.ni.com/nl-nl/innovations/white-papers/06/pid-theory-explained.html},
}
@article{Hong2018,
   abstract = {This article provides a perspective on control and operations for biopharmaceutical manufacturing. Challenges and opportunities are described for (1) microscale technologies for high-speed continuous processing, (2) plug-and-play modular unit operations with integrated monitoring and control systems, (3) dynamic modeling of unit operations and entire biopharmaceutical manufacturing plants to support process development and plant-wide control, and (4) model-based control technologies for optimizing startup, changeover, and shutdown. A challenge is the ability to simultaneously address the uncertainties, nonlinearities, time delays, non-minimum phase behavior, constraints, spatial distributions, and mixed continuous-discrete operations that arise in biopharmaceutical operations. The design of adaptive and hybrid control strategies is discussed. Process data analytics and grey-box modeling methods are needed to deal with the heterogeneity and tensorial dimensionality of biopharmaceutical data. Novel bioseparations as discussed as a potential cost-effective unit operation, with a discussion of challenges for the widespread application of crystallization to therapeutic proteins.},
   author = {Moo Sun Hong and Kristen A. Severson and Mo Jiang and Amos E. Lu and J. Christopher Love and Richard D. Braatz},
   doi = {10.1016/J.COMPCHEMENG.2017.12.007},
   issn = {0098-1354},
   journal = {Computers \& Chemical Engineering},
   keywords = {Biomanufacturing,Biopharmaceutical manufacturing,Biopharmaceuticals,Continuous manufacturing,Process data analytics,Protein crystallization},
   month = {2},
   pages = {106-114},
   publisher = {Pergamon},
   title = {Challenges and opportunities in biopharmaceutical manufacturing control},
   volume = {110},
   year = {2018},
}
@web_page{capeopen,
   title = {the CAPE-OPEN Laboratories Network | Expanding Process Modelling Capability through Software Interoperability Standards},
   url = {https://www.colan.org/},
}
@web_page{fmi,
   title = {Functional Mock-up Interface},
   url = {https://fmi-standard.org/},
}
@web_page{fmidoc,
   title = {Functional Mock-up Interface Specification},
   url = {https://fmi-standard.org/docs/3.0-dev/},
}
@web_page{ptolemy,
   title = {Ptolemy Project Home Page},
   url = {https://ptolemy.berkeley.edu/},
}
@web_page{sysml,
   title = {{SysML} Open Source Project - What is {SysML}? Who created it?},
   url = {https://sysml.org/},
}
@article{Tripakis2014,
   author = {Stavros Tripakis and Edward A Lee},
   title = {Fundamental Algorithms for System Modeling, Analysis, and Optimization},
   year = {2014},
}
@web_page{sysmlspec,
   title = {{OMG} System Modeling Language Specification Version 1.0},
   url = {https://www.omg.org/spec/SysML/1.0/About-SysML/},
}
@web_page{Kubernetes,
   title = {Kubernetes},
   url = {https://kubernetes.io/},
}
@article{Helgers2022,
   abstract = {The development of new biologics is becoming more challenging due to global competition and increased requirements for process understanding and assured quality in regulatory approval. As a result, there is a need for predictive, mechanistic process models. These reduce the resources and time required in process development, generating understanding, expanding the possible operating space, and providing the basis for a digital twin for automated process control. Monoclonal antibodies are an important representative of industrially produced biologics that can be used for a wide range of applications. In this work, the validation of a mechanistic process model with respect to sensitivity, as well as accuracy and precision, is presented. For the investigated process conditions, the concentration of glycine, phenylalanine, tyrosine, and glutamine have been identified as significant influencing factors for product formation via statistical evaluation. Cell growth is, under the investigated process conditions, significantly dependent on the concentration of glucose within the investigated design space. Other significant amino acids were identified. A Monte Carlo simulation was used to simulate the cultivation run with an optimized medium resulting from the sensitivity analysis. The precision of the model was shown to have a 95% confidence interval. The model shown here includes the implementation of cell death in addition to models described in the literature.},
   author = {Heribert Helgers and Axel Schmidt and Jochen Strube},
   doi = {10.3390/pr10020316},
   issn = {2227-9717},
   issue = {2},
   journal = {Processes},
   title = {Towards Autonomous Process Control—Digital Twin for CHO Cell-Based Antibody Manufacturing Using a Dynamic Metabolic Model},
   volume = {10},
   url = {https://www.mdpi.com/2227-9717/10/2/316},
   year = {2022},
}
@article{Feidl2020,
   abstract = {Integrated continuous manufacturing is entering the biopharmaceutical industry. The main drivers range from improved economics, manufacturing flexibility, and more consistent product quality. However, studies on fully integrated production platforms have been limited due to the higher degree of system complexity, limited process information, disturbance, and drift sensitivity, as well as difficulties in digital process integration. In this study, we present an automated end-to-end integrated process consisting of a perfusion bioreactor, CaptureSMB, virus inactivation (VI), and two polishing steps to produce an antibody from an instable cell line. A supervisory control and data acquisition (SCADA) system was developed, which digitally integrates unit operations and analyzers, collects and centrally stores all process data, and allows process-wide monitoring and control. The integrated system consisting of bioreactor and capture step was operated initially for 4 days, after which the full end-to-end integrated run with no interruption lasted for 10 days. In response to decreasing cell-specific productivity, the supervisory control adjusted the loading duration of the capture step to obtain high capacity utilization without yield loss and constant antibody quantity for subsequent operations. Moreover, the SCADA system coordinated VI neutralization and discharge to enable constant loading conditions on the polishing unit. Lastly, the polishing was sufficiently robust to cope with significantly increased aggregate levels induced on purpose during virus inactivation. It is demonstrated that despite significant process disturbances and drifts, a robust process design and the supervisory control enabled constant (optimum) process performance and consistent product quality.},
   author = {Fabian Feidl and Sebastian Vogg and Moritz Wolf and Matevz Podobnik and Caterina Ruggeri and Nicole Ulmer and Ruben Wälchli and Jonathan Souquet and Hervé Broly and Alessandro Butté and Massimo Morbidelli},
   doi = {10.1002/BIT.27296},
   issn = {1097-0290},
   issue = {5},
   journal = {Biotechnology and Bioengineering},
   keywords = {PAT,downstream processing,integrated continuous biomanufacturing,perfusion cell culture,supervisory control},
   month = {5},
   pages = {1367-1380},
   pmid = {32022243},
   publisher = {John Wiley & Sons, Ltd},
   title = {Process-wide control and automation of an integrated continuous manufacturing platform for antibodies},
   volume = {117},
   url = {https://onlinelibrary.wiley.com/doi/full/10.1002/bit.27296},
   year = {2020},
}
@article{Hugues2020,
   abstract = {The engineering of Cyber-Physical Systems (CPS) requires a large set of expertise to capture the system requirements and to derive a correct solution. Model-based Engineering and DevOps aim to efficiently deliver software with increased quality. Model-based Engineering relies on models as first-class artifacts to analyze, simulate, and ultimately generate parts of a system. DevOps focuses on software engineering activities, from early development to integration, and then improvement through the monitoring of the system at run-time. We claim these can be efficiently combined to improve the engineering process of CPS. In this paper, we present TwinOps, a process that unifies Model-based Engineering, Digital Twins, and DevOps practice in a uniform workflow. TwinOps illustrates how to leverage several best practices in MBE and DevOps for the engineering Cyber-Physical systems. We illustrate our contribution using a Digital Twins case study to illustrate TwinOps benefits, combining AADL and Modelica models, and an IoT platform.},
   author = {Jerome Hugues and Anton Hristosov and John J. Hudak and Joe Yankel},
   doi = {10.1145/3417990.3421446},
   isbn = {9781450381352},
   journal = {Proceedings - 23rd ACM/IEEE International Conference on Model Driven Engineering Languages and Systems, MODELS-C 2020 - Companion Proceedings},
   month = {10},
   pages = {668},
   publisher = {Association for Computing Machinery, Inc},
   title = {{TwinOps} - {DevOps} meets model-based engineering and digital twins for the engineering of {CPS}},
   url = {https://doi.org/10.1145/3417990.3421446},
   year = {2020},
}
@article{Lee2004,
   abstract = {Most current hardware engineering practice is deeply rooted in discrete-event modeling and synchronous design. Most current software engineering is deeply rooted in procedural abstractions. The latter says little about concurrency and temporal properties, whereas the...},
   author = {Edward A. Lee and Stephen Neuendorffer},
   doi = {10.1007/978-1-4020-8052-4_2},
   journal = {Formal Methods and Models for System Design},
   keywords = {Actor-oriented design,Ptolemy,behavioral polymorphism,behavioral types,code generation,component specialization,parameterization},
   pages = {33-56},
   publisher = {Springer, Boston, MA},
   title = {Actor-Oriented Models for Codesign},
   url = {https://link.springer.com/chapter/10.1007/978-1-4020-8052-4_2},
   year = {2004},
}
@article{Ebert2016,
   abstract = {Building on lean and agile practices, DevOps means end-to-end automation in software development and delivery. Hardly anybody will be able to approach it with a cookbook-style approach, but most developers will benefit from better connecting the previously isolated silos of development and operations. Many DevOps tools exist that can help them do this.},
   author = {Christof Ebert and Gorka Gallardo and Josune Hernantes and Nicolas Serrano},
   doi = {10.1109/MS.2016.68},
   issn = {19374194},
   issue = {3},
   journal = {IEEE Software},
   keywords = {AWS,Amazon Web Services,Ansible,Bamboo,Cacti,Chef,DevOps,Gradle,Graylog2,Jenkins,Logging,Loggly,Maven,Nagios,New Relic,Puppet,TeamCity,apache Ant,configuration management,continuous integration,microservices,software development,software engineering},
   month = {5},
   pages = {94-100},
   publisher = {IEEE Computer Society},
   title = {DevOps},
   volume = {33},
   year = {2016},
}
@article{Macchi2018,
   abstract = {This work is an explorative study to reflect on the role of digital twins to support decisionmaking in asset lifecycle management. The study remarks the current convergence of needs for decision support from Asset Management and of potentials for decision support offered by Digital Twin modeling. The importance of digital twins is evident through state of the art as well as practical use case analysis.},
   author = {Marco Macchi and Irene Roda and Elisa Negri and Luca Fumagalli},
   doi = {10.1016/J.IFACOL.2018.08.415},
   issn = {2405-8963},
   issue = {11},
   journal = {IFAC-PapersOnLine},
   keywords = {Asset management,Decision support,Digital twin,Lifecycle management},
   month = {1},
   pages = {790-795},
   publisher = {Elsevier},
   title = {Exploring the role of Digital Twin for Asset Lifecycle Management},
   volume = {51},
   year = {2018},
}
@article{Lim2019,
   abstract = {With the rapid advancement of cyber-physical systems, Digital Twin (DT) is gaining ever-increasing attention owing to its great capabilities to realize Industry 4.0. Enterprises from different fields are taking advantage of its ability to simulate real-time working conditions and perform intelligent decision-making, where a cost-effective solution can be readily delivered to meet individual stakeholder demands. As a hot topic, many approaches have been designed and implemented to date. However, most approaches today lack a comprehensive review to examine DT benefits by considering both engineering product lifecycle management and business innovation as a whole. To fill this gap, this work conducts a state-of-the art survey of DT by selecting 123 representative items together with 22 supplementary works to address those two perspectives, while considering technical aspects as a fundamental. The systematic review further identifies eight future perspectives for DT, including modular DT, modeling consistency and accuracy, incorporation of Big Data analytics in DT models, DT simulation improvements, VR integration into DT, expansion of DT domains, efficient mapping of cyber-physical data and cloud/edge computing integration. This work sets out to be a guide to the status of DT development and application in today’s academic and industrial environment.},
   author = {Kendrik Yan Hong Lim and Pai Zheng and Chun Hsien Chen},
   doi = {10.1007/S10845-019-01512-W},
   isbn = {0123456789},
   issn = {1572-8145},
   issue = {6},
   journal = {Journal of Intelligent Manufacturing 2019 31:6},
   keywords = {Control,Machines,Manufacturing,Mechatronics,Processes,Production,Robotics,Tools},
   month = {11},
   pages = {1313-1337},
   publisher = {Springer},
   title = {A state-of-the-art survey of Digital Twin: techniques, engineering product lifecycle management and business innovation perspectives},
   volume = {31},
   url = {https://link.springer.com/article/10.1007/s10845-019-01512-w},
   year = {2019},
}
@article{Ansari2020,
   abstract = {Original equipment manufacturers (OEMs), machine operators and suppliers are three main stakeholders in today's industrial asset management lifecycle. Considering high rate of investment and prices to purchase and operationalize secured IoT-based production systems, insurance companies explore contributing into industrial asset management lifecycle by providing new products to transparently monitor online condition and operation status of industrial machines, and accordingly adjust the insurance prices and offers. Hence, joint efforts and cross-disciplinary research is vital to introduce a new insurance product, i.e. failure-based insurance, gaining benefits from AI technologies and knowledge-based maintenance approaches. This paper provides insights into the aforementioned problem, and presents the work-in-progress research on failure-based insurance model aided by Digital Twin technology. Finally, the practical implications and future research directions are outlined.},
   author = {Fazel Ansari and Steffen Nixdorf and Wilfried Sihn},
   doi = {10.1016/J.IFACOL.2020.11.063},
   issn = {2405-8963},
   issue = {3},
   journal = {IFAC-PapersOnLine},
   keywords = {AI for business and economy,Cyber-Physical Production Systems,Data driven decision making,Industry 4.0,Intelligent maintenance systems,Knowledge discovery},
   month = {1},
   pages = {295-300},
   publisher = {Elsevier},
   title = {Insurability of Cyber Physical Production Systems: How Does Digital Twin Improve Predictability of Failure Risk?},
   volume = {53},
   year = {2020},
}
@article{Gurdur2022 ,
   abstract = {There is a critical need to make infrastructure systems more efficient, resilient, and sustainable. Infrastructure systems provide the basis for everyday life and enable the flow of goods, information, and services within urban and regional settings. Providing data-centric solutions to improve this flow is essential. This can only be achieved if we manage to transform passive infrastructure assets into cyber-physical systems. Digital twins bring the opportunity to turn passive infrastructure assets into data-centric systems of systems. This article aims to provide a summary of existing digital twin architectures and exemplify a digital twin design and implementation. To this end, a literature review of digital twin architecture is presented in addition to a case study of a digital twin implementation in smart infrastructure. The case study focuses on a digital twin implementation of a bridge and describes in detail the physical, cyber, integration, and service layers of this implementation. Later in the article, we discuss the learnings from this case study under three main categories – systems perspective, information perspective, and organisational perspective. The findings show the importance of acquiring a systems perspective when designing digital twins today to enable interoperable systems of systems in the future. Furthermore, the findings highlight the vital necessity of data and information management while also considering the multidisciplinary aspects of digital twin design and implementation.},
   author = {Didem Gürdür Broo and Miguel Bravo-Haro and Jennifer Schooling},
   doi = {10.1016/J.AUTCON.2022.104171},
   issn = {0926-5805},
   journal = {Automation in Construction},
   keywords = {Data,Digital twins,Infrastructure,Resilience,Smart infrastructure},
   month = {4},
   pages = {104171},
   publisher = {Elsevier},
   title = {Design and implementation of a smart infrastructure digital twin},
   volume = {136},
   year = {2022},
}
@article{Cai2021,
   abstract = {During the manufacturing process of aircraft, quality deviation problems inevitably occur due to the high complexity of aircraft design, manufacturing errors, tooling mistakes, human factors, environmental influences, design defects, and other factors. The current quality deviation control system of civil aircraft suffers from two problems: (1) quality deviation control data are scattered in more than 100 management systems, and it is difficult to extract quality data-related information from the whole life cycle of the aircraft involving the main manufacturer and each supplier and (2) there is a lack of quality data analysis and a closed-loop information-physics fusion system for quality deviation control. Thus, it is difficult to locate the quality deviation problems and it takes a long time to deal with these problems as well. In this paper, a digital twin-based quality deviation control model is proposed. Through the digital twin modeling based on asset management shell technology, the multi-source and heterogeneous quality deviation data can be extracted and integrated. Furthermore, to deal with the second problem, a quality deviation system has been built based on digital twin. In this system, the aircraft quality deviation data can be analyzed by the FP-growth association rule algorithm and the results are provided through the system to guide the assembly site, improving the efficiency and accuracy of quality problem-solving in the physical world. In addition, a case study is stated, where the proposed approach is applied to deal with the aircraft quality deviation problems.},
   author = {Hongxia Cai and Jiamin Zhu and Wei Zhang},
   doi = {10.1115/1.4050376/1102047},
   issn = {15309827},
   issue = {3},
   journal = {Journal of Computing and Information Science in Engineering},
   keywords = {Aircraft assembly,Asset management shell,Data-driven engineering,Digital twin,FP-growth association analysis,Information management,Physics-based simulations,Quality deviation control},
   month = {6},
   publisher = {American Society of Mechanical Engineers (ASME)},
   title = {Quality deviation control for aircraft using digital twin},
   volume = {21},
   url = {https://asmedigitalcollection.asme.org/computingengineering/article/21/3/031008/1102047/Quality-Deviation-Control-for-Aircraft-Using},
   year = {2021},
}
@article{Boschert2016,
   abstract = {The vision of the Digital Twin itself refers to a comprehensive physical and functional description of a component, product or system, which includes more or less all information which could be useful in all-the current and subsequent-lifecycle phases. In this chapter we focus on the simulation aspects of the Digital Twin. Today, modelling and simulation is a standard process in system development, e.g. to support design tasks or to validate system properties. During operation and for service first simulation-based solutions are realized for optimized operations and failure prediction. In this sense, simulation merges the physical and virtual world in all life cycle phases. Current practice already enables the users (designer, SW/HW developers, test engineers, operators, maintenance personnel, etc) to master the complexity of mechatronic systems.},
   author = {Stefan Boschert and Roland Rosen},
   doi = {10.1007/978-3-319-32156-1_5/FIGURES/4},
   isbn = {9783319321561},
   journal = {Mechatronic Futures: Challenges and Solutions for Mechatronic Systems and Their Designers},
   month = {1},
   pages = {59-74},
   publisher = {Springer International Publishing},
   title = {Digital twin-the simulation aspect},
   url = {https://link.springer.com/chapter/10.1007/978-3-319-32156-1_5},
   year = {2016},
}
@article{Jiang2021,
   abstract = {Digital twin (DT) is a virtual mirror (representation) of a physical world or a system along its lifecycle. As for a complex discrete manufacturing system (DMS), it is a digital model for emulating or reproducing the functions or actions of a real manufacturing system by giving the system simulation information or directly driven by a real system with proper connections between the DT model and the real-world system. It is a key building block for smart factory and manufacturing under the Industry 4.0 paradigm. The key research question is how to effectively create a DT model during the design stage of a complex manufacturing system and to make it usable throughout the system's lifecycle such as the production stage. Given that there are some existing discussions on DT framework development, this paper focuses on the modeling methods for rapidly creating a virtual model and the connection implementation mechanism between a physical world production system at a workshop level and its mirrored virtual model. To reach above goals, in this paper, the discrete event system (DES) modeling theory is applied to the three-dimension DT model. First, for formally representing a manufacturing system and creating its virtual model, seven basic elements: controller, executor, processor, buffer, flowing entity, virtual service node and logistics path of a DMS have been identified and the concept of the logistics path network and the service cell is introduced to uniformly describe a manufacturing system. Second, for implementing interconnection and interaction, a new interconnection and data interaction mechanism between the physical system and its virtual model for through-life applications has been designed. With them, each service cell consists of seven elements and encapsulates input/output information and control logic. All the discrete cells are constructed and mapped onto different production-process-oriented digital manufacturing modules by integrating logical, geometric and data models. As a result, the virtual-physical connection is realized to form a DT model. The proposed virtual modeling method and the associated connection mechanism have been applied to a real-world workshop DT to demonstrate its practicality and usefulness.},
   author = {Haifan Jiang and Shengfeng Qin and Jianlin Fu and Jian Zhang and Guofu Ding},
   doi = {10.1016/J.JMSY.2020.05.012},
   issn = {0278-6125},
   journal = {Journal of Manufacturing Systems},
   keywords = {Cyber-physical system,Digital twin,Discrete event system modeling theory,Virtual-physical connection},
   month = {1},
   pages = {36-51},
   publisher = {Elsevier},
   title = {How to model and implement connections between physical and virtual models for digital twin application},
   volume = {58},
   year = {2021},
}
@article{Zhou2021,
   abstract = {The world's increasing population requires the process industry to produce food, fuels, chemicals, and consumer products in a more efficient and sustainable way. Functional process materials lie at the heart of this challenge. Traditionally, new advanced materials are found empirically or through trial-and-error approaches. As theoretical methods and associated tools are being continuously improved and computer power has reached a high level, it is now efficient and popular to use computational methods to guide material selection and design. Due to the strong interaction between material selection and the operation of the process in which the material is used, it is essential to perform material and process design simultaneously. Despite this significant connection, the solution of the integrated material and process design problem is not easy because multiple models at different scales are usually required. Hybrid modeling provides a promising option to tackle such complex design problems. In hybrid modeling, the material properties, which are computationally expensive to obtain, are described by data-driven models, while the well-known process-related principles are represented by mechanistic models. This article highlights the significance of hybrid modeling in multiscale material and process design. The generic design methodology is first introduced. Six important application areas are then selected: four from the chemical engineering field and two from the energy systems engineering domain. For each selected area, state-of-the-art work using hybrid modeling for multiscale material and process design is discussed. Concluding remarks are provided at the end, and current limitations and future opportunities are pointed out.},
   author = {Teng Zhou and Rafiqul Gani and Kai Sundmacher},
   doi = {10.1016/J.ENG.2020.12.022},
   issn = {2095-8099},
   issue = {9},
   journal = {Engineering},
   keywords = {Data-driven,Hybrid modeling,Machine learning,Material design,Process optimization,Surrogate model},
   month = {9},
   pages = {1231-1238},
   publisher = {Elsevier},
   title = {Hybrid Data-Driven and Mechanistic Modeling Approaches for Multiscale Material and Process Design},
   volume = {7},
   year = {2021},
}
@article{Grieves2019,
   abstract = {Complex Systems Engineering: Theory and Practice represents state-of-the-art thought leadership on system complexity for aerospace and aviation, where breakthrough paradigms and strategies are sore...},
   author = {Michael W. Grieves},
   doi = {10.2514/5.9781624105654.0175.0200},
   journal = {Complex Systems Engineering: Theory and Practice},
   keywords = {Aerospace Manufacturers,Aerospace Systems Complexity,Air Force Wright Aeronautical Laboratories,Center for Advanced Aviation System Development,Commercial Aircraft,Department of Transportation,Enterprise Resource Planning,International Organization for Standardization,Model Based System Engineering,Simultaneous Engineering},
   month = {1},
   pages = {175-200},
   publisher = {American Institute of Aeronautics and Astronautics, Inc.},
   title = {Virtually Intelligent Product Systems: Digital and Physical Twins},
   url = {https://arc.aiaa.org/doi/pdf/10.2514/5.9781624105654.0175.0200},
   year = {2019},
}
@web_page{azureiot,
   author = {Microsoft},
   title = {Azure {IoT} – Internet of Things Platform},
   url = {https://azure.microsoft.com/en-us/overview/iot},
}
@article{Verma2016,
   abstract = {Machine-to-Machine (M2M) communication is a promising technology for next generation communication systems. This communication paradigm facilitates ubiquitous communications with full mechanical automation, where a large number of intelligent devices connected by wired/wireless links, interact with each other without direct human intervention. As a result, M2M communication finds applications in wide areas such as smart grids, e-healthcare, home area networks, intelligent transportation systems, environmental monitoring, smart cities, and industrial automation. However, distinctive features in M2M communications form different challenges from those in human-to-human communications. These challenges need to be addressed, or otherwise it is not easy for this paradigm to gain trust of people. To understand M2M communications deeply, this paper presents a comprehensive review of M2M communication technology in terms of its system model architecture proposed by different standards developing organizations. This mainly includes 3GPP, ETSI, and oneM2M. Further, we have investigated distinctive features of various M2M applications and their supporting attributes, the M2M data traffic and their characterization, various M2M standardization bodies and their unique tasks, and potential M2M communication challenges and their proposed state-of-the-art solutions, followed by future research directions.},
   author = {Pawan Kumar Verma and Rajesh Verma and Arun Prakash and Ashish Agrawal and Kshirasagar Naik and Rajeev Tripathi and Maazen Alsabaan and Tarek Khalifa and Tamer Abdelkader and Abdulhakim Abogharaf},
   doi = {10.1016/J.JNCA.2016.02.016},
   issn = {1084-8045},
   journal = {Journal of Network and Computer Applications},
   keywords = {Automation,Machine-to-Machine communication,Quality of Service,Self-configuration,Self-orientation,Ubiquitous},
   month = {5},
   pages = {83-105},
   publisher = {Academic Press},
   title = {Machine-to-Machine (M2M) communications: A survey},
   volume = {66},
   year = {2016},
}
@web_page{boeing,
   author = {Boeing},
   month = {3},
   title = {Key Findings on Airplane Economic Life},
   url = {https://www.boeing.com/assets/pdf/commercial/aircraft_economic_life_whitepaper.pdf},
   year = {2013},
}
@article{Zheng2018,
   abstract = {Information and communication technology is undergoing rapid development, and many disruptive technologies, such as cloud computing, Internet of Things, big data, and artificial intelligence, have emerged. These technologies are permeating the manufacturing industry and enable the fusion of physical and virtual worlds through cyber-physical systems (CPS), which mark the advent of the fourth stage of industrial production (i.e., Industry 4.0). The widespread application of CPS in manufacturing environments renders manufacturing systems increasingly smart. To advance research on the implementation of Industry 4.0, this study examines smart manufacturing systems for Industry 4.0. First, a conceptual framework of smart manufacturing systems for Industry 4.0 is presented. Second, demonstrative scenarios that pertain to smart design, smart machining, smart control, smart monitoring, and smart scheduling, are presented. Key technologies and their possible applications to Industry 4.0 smart manufacturing systems are reviewed based on these demonstrative scenarios. Finally, challenges and future perspectives are identified and discussed.},
   author = {Pai Zheng and Honghui wang and Zhiqian Sang and Ray Y. Zhong and Yongkui Liu and Chao Liu and Khamdi Mubarok and Shiqiang Yu and Xun Xu},
   doi = {10.1007/S11465-018-0499-5},
   issn = {2095-0241},
   issue = {2},
   journal = {Frontiers of Mechanical Engineering 2018 13:2},
   keywords = {Internet of Things,Mechanical Engineering,big data analytics,cyber-physical systems,framework,smart manufacturing systems},
   month = {1},
   pages = {137-150},
   publisher = {Springer},
   title = {Smart manufacturing systems for Industry 4.0: Conceptual framework, scenarios, and future perspectives},
   volume = {13},
   url = {https://link.springer.com/article/10.1007/s11465-018-0499-5},
   year = {2018},
}
@article{Kwon2022,
   abstract = {OBJECTIVE: Smart hospitals involve the application of recent information and communications technology (ICT) innovations to medical services; however, the concept of a smart hospital has not been rigorously defined. In this study, we aimed to derive the definition and service types of smart hospitals and investigate cases of each type. METHODS: A literature review was conducted regarding the background and technical characteristics of smart hospitals. On this basis, we conducted a focus group interview with experts in hospital information systems, and ultimately derived eight smart hospital service types. RESULTS: Smart hospital services can be classified into the following types: services based on location recognition and tracking technology that measures and monitors the location information of an object based on short-range communication technology; high-speed communication network-based services based on new wireless communication technology; Internet of Things-based services that connect objects embedded with sensors and communication functions to the internet; mobile health services such as mobile phones, tablets, and wearables; artificial intelligence-based services for the diagnosis and prediction of diseases; robot services provided on behalf of humans in various medical fields; extended reality services that apply hyper-realistic immersive technology to medical practice; and telehealth using ICT. CONCLUSIONS: Smart hospitals can influence health and medical policies and create new medical value by defining and quantitatively measuring detailed indicators based on data collected from existing hospitals. Simultaneously, appropriate government incentives, consolidated interdisciplinary research, and active participation by industry are required to foster and facilitate smart hospitals.},
   author = {Hyuktae Kwon and Sunhee An and Ho-Young Lee and Won Chul Cha and Sungwan Kim and Minwoo Cho and Hyoun-Joong Kong},
   doi = {10.4258/hir.2022.28.1.3},
   edition = {2022/01/31},
   issn = {2093-3681},
   issue = {1},
   journal = {Healthcare informatics research},
   keywords = {Digital Technology,Health Services Administration,Hospital Design and Construction,Hospital Planning,Meaningful Use},
   month = {1},
   pages = {3-15},
   pmid = {35172086},
   publisher = {Korean Society of Medical Informatics},
   title = {Review of Smart Hospital Services in Real Healthcare Environments},
   volume = {28},
   url = {https://pubmed.ncbi.nlm.nih.gov/35172086},
   year = {2022},
}
@article{Duan2019,
   abstract = {Artificial intelligence (AI) has been in existence for over six decades and has experienced AI winters and springs. The rise of super computing power and Big Data technologies appear to have empowered AI in recent years. The new generation of AI is rapidly expanding and has again become an attractive topic for research. This paper aims to identify the challenges associated with the use and impact of revitalised AI based systems for decision making and offer a set of research propositions for information systems (IS) researchers. The paper first provides a view of the history of AI through the relevant papers published in the International Journal of Information Management (IJIM). It then discusses AI for decision making in general and the specific issues regarding the interaction and integration of AI to support or replace human decision makers in particular. To advance research on the use of AI for decision making in the era of Big Data, the paper offers twelve research propositions for IS researchers in terms of conceptual and theoretical development, AI technology-human interaction, and AI implementation.},
   author = {Yanqing Duan and John S. Edwards and Yogesh K. Dwivedi},
   doi = {10.1016/J.IJINFOMGT.2019.01.021},
   issn = {0268-4012},
   journal = {International Journal of Information Management},
   keywords = {AI,Artificial intelligence,Big data,Cognitive computing,Decision making,Expert system,Machine learning,Recommender system,Research agenda},
   month = {10},
   pages = {63-71},
   publisher = {Pergamon},
   title = {Artificial intelligence for decision making in the era of Big Data – evolution, challenges and research agenda},
   volume = {48},
   year = {2019},
}
@web_page{Thingsboard,
   author = {ThingsBoard},
   title = {{ThingsBoard} - Open-source {IoT} Platform},
   url = {https://thingsboard.io/},
}
@web_page{OMEdit ,
   author = {OpenModelica},
   title = {{OMEdit} – {OpenModelica} Connection Editor},
   url = {https://openmodelica.org/doc/OpenModelicaUsersGuide/latest/omedit.html},
}
@article{SSP,
   abstract = {This document is the first public release of the System Structure and Parameterization Standard (SSP). This constitutes a standard of the Modelica Association. On the Downloads page (https://ssp-standard.org/downloads), this specification, as well as supporting XML schema files are provided. Contact the SSP development group at map-ssp_projectlead@googlegroups.com.},
   author = "{Modelica Association}",
   keywords = {Public},
   title = {System Structure and Parameterization},
   url = {http://www.opensource.org/licenses/bsd-license.html},
   year = {2019},
}
@web_page{GitHubActions,
   author = {GitHub},
   title = {{GitHub Actions} Documentation},
   url = {https://docs.github.com/en/actions},
}
@web_page{Docker,
   author = "{Docker Inc}",
   title = {Docker Documentation},
   url = {https://docs.docker.com/},
}
@web_page{Kafka,
   author = {Apache},
   title = {{Apache Kafka}},
   url = {https://kafka.apache.org/documentation/},
}
@web_page{Simulink,
   author = {Mathworks},
   title = {Simulink - Simulation and Model-Based Design},
   url = {https://nl.mathworks.com/products/simulink.html},
}
@web_page{CATIA,
   author = "{Dassault Systèmes}",
   title = {CATIA},
   url = {https://www.3ds.com/products-services/catia/},
}


@article{Bordeleau2020,
   abstract = {Digital Twins have emerged since the beginning of this millennium to better support the management of systems based on (real-time) data collected in different parts of the operating systems. Digital Twins have been successfully used in many application domains, and thus, are considered as an important aspect of Model-Based Systems Engineering (MBSE). However, their development, maintenance, and evolution still face major challenges, in particular: (i) the management of heterogeneous models from different disciplines, (ii) the bi-directional synchronization of digital twins and the actual systems, and (iii) the support for collaborative development throughout the complete life-cycle. In the last decades, the Model-Driven Engineering (MDE) community has investigated these challenges in the context of software systems. Now the question arises, which results may be applicable for digital twin engineering as well. In this paper, we identify various MDE techniques and technologies which may contribute to tackle the three mentioned digital twin challenges as well as outline a set of open MDE research challenges that need to be addressed in order to move towards a digital twin engineering discipline.},
   author = {Francis Bordeleau and Benoit Combemale and Romina Eramo and Mark van den Brand and Manuel Wimmer},
   doi = {10.1007/978-3-030-58167-1_4/FIGURES/1},
   isbn = {9783030581664},
   issn = {18650937},
   journal = {Communications in Computer and Information Science},
   keywords = {Digital twins,Heterogeneous modeling,Modeling languages},
   pages = {43-54},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Towards Model-Driven Digital Twin Engineering: Current Opportunities and Future Challenges},
   volume = {1262 CCIS},
   url = {https://link.springer.com/chapter/10.1007/978-3-030-58167-1_4},
   year = {2020},
}

@web_page{IBMSYSML,
   author = {IBM},
   title = {Engineering Systems Design Rhapsody},
   url = {https://www.ibm.com/products/systems-design-rhapsody},
}

@book{Kossiakoff2011,
  title={Systems engineering principles and practice},
  author={Kossiakoff, Alexander and Sweet, William N and Seymour, Samuel J and Biemer, Steven M},
  volume={83},
  year={2011},
  publisher={John Wiley \& Sons}
}



